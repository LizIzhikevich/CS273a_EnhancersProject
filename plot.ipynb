{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from evaluator import binarize_Y, interpolate_precision, get_interpolated_avg_precision\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up result files and data for analysis\n",
    "outdir = 'random_plots/'\n",
    "best_predfile = 'best_results/best_random/Y_test_pred.pickle'\n",
    "base_predfile = 'random_results/nb_2019_03_12_09_34/Y_test_pred.pickle'\n",
    "actual_Yfile = 'best_results/best_random/Y_test.pickle'\n",
    "best_mdl = 'SVM'\n",
    "base_mdl = 'NB'\n",
    "\n",
    "best_pred = pickle.load(open(best_predfile, 'rb'))\n",
    "base_pred = pickle.load(open(base_predfile, 'rb'))\n",
    "actual_Y = pickle.load(open(actual_Yfile, 'rb'))\n",
    "\n",
    "Y_predicted_lst = [best_pred, base_pred]\n",
    "mdlname_lst = [best_mdl, base_mdl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Labels and groupings to analyze\n",
    "GEN_LABELS = [(0,1,2), 0, 1, 2, 3, 4, 5]\n",
    "GEN_LAB_STR = {(0,1,2): 'brain', 0: 'forebrain', 1: 'midbrain', 2: 'hindbrain', 3: 'heart', 4: 'limb', 5: 'others'}\n",
    "FINE_LABELS = [0, 1, 2, 3, 4]\n",
    "FINE_LAB_STR = {0: 'forebrain', 1: 'midbrain', 2: 'hindbrain', 3: 'heart', 4: 'limb'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot ROC or precision-recall curve for one vs. rest comparison\n",
    "def plot_ovr(Y_actual, Y_predicted_lst, mdlname_lst, pos_labels_lst, label_to_str, outdir, metric = 'auc'):\n",
    "    outdir += 'ovr/'\n",
    "    if metric == 'auc':\n",
    "        title = 'ROC curve'\n",
    "    elif metric == 'prc':\n",
    "        title = 'Precision-recall curve'\n",
    "\n",
    "    for pos_labels in pos_labels_lst:\n",
    "        labelstr = label_to_str[pos_labels]\n",
    "        plot_title = title + ' (%s vs. others)' % labelstr\n",
    "\n",
    "        if type(pos_labels) == tuple:\n",
    "            pos_labels = list(pos_labels)\n",
    "        else:\n",
    "            pos_labels = [pos_labels]\n",
    "\n",
    "        plt.clf()\n",
    "        plt.title(plot_title)\n",
    "        for i in range(len(Y_predicted_lst)):\n",
    "            bin_Y_actual, bin_Y_pred = binarize_Y(Y_actual, Y_predicted_lst[i], pos_labels)\n",
    "            if metric == 'auc':\n",
    "                metric_estimate = roc_auc_score(bin_Y_actual, bin_Y_pred)\n",
    "                x, y, threshold = roc_curve(bin_Y_actual, bin_Y_pred)\n",
    "                plt.plot(x, y, label='%s (%.2f)' % (mdlname_lst[i], metric_estimate))\n",
    "            elif metric == 'prc':\n",
    "                metric_estimate = get_interpolated_avg_precision(bin_Y_actual, bin_Y_pred)\n",
    "                x, y, threshold = precision_recall_curve(bin_Y_actual, bin_Y_pred)\n",
    "                x = interpolate_precision(x)\n",
    "                plt.plot(x, y, label='%s (%.2f)' % (mdlname_lst[i], metric_estimate))\n",
    "        if metric == 'auc':\n",
    "            plt.plot([0,1], [0,1], color='black')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.legend(loc='lower right')\n",
    "        elif metric == 'prc':\n",
    "            plt.ylabel('Average precision')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.legend(loc='upper right')\n",
    "        plt.savefig('%s%s_%s' % (outdir, metric, labelstr))\n",
    "\n",
    "# Plot ROC or precision-recall curve for one vs. one comparison\n",
    "def plot_ovo(Y_actual, Y_predicted_lst, mdlname_lst, labels, label_to_str, outdir, metric = 'auc'):\n",
    "    outdir += 'ovo/'\n",
    "    if metric == 'auc':\n",
    "        title = 'ROC curve'\n",
    "    elif metric == 'prc':\n",
    "        title = 'Precision-recall curve'\n",
    "\n",
    "    for pair in list(itertools.combinations(labels, 2)):\n",
    "        pos_label = pair[0]\n",
    "        neg_label = pair[1]\n",
    "        pos_str = label_to_str[pos_label]\n",
    "        neg_str = label_to_str[neg_label]\n",
    "        \n",
    "        plot_title = title + ' (%s vs. %s)' % (pos_str, neg_str)\n",
    "\n",
    "        plt.clf()\n",
    "        plt.title(plot_title)\n",
    "        for i in range(len(Y_predicted_lst)):\n",
    "            bin_Y_actual, bin_Y_pred = binarize_Y(Y_actual, Y_predicted_lst[i], [pos_label], [neg_label])\n",
    "            if metric == 'auc':\n",
    "                metric_estimate = roc_auc_score(bin_Y_actual, bin_Y_pred)\n",
    "                x, y, threshold = roc_curve(bin_Y_actual, bin_Y_pred)\n",
    "                plt.plot(x, y, label='%s (%.2f)' % (mdlname_lst[i], metric_estimate))\n",
    "            elif metric == 'prc':\n",
    "                metric_estimate = get_interpolated_avg_precision(bin_Y_actual, bin_Y_pred)\n",
    "                x, y, threshold = precision_recall_curve(bin_Y_actual, bin_Y_pred)\n",
    "                x = interpolate_precision(x)\n",
    "                plt.plot(x, y, label='%s (%.2f)' % (mdlname_lst[i], metric_estimate))\n",
    "        if metric == 'auc':\n",
    "            plt.plot([0,1], [0,1], color='black')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.legend(loc='lower right')\n",
    "        elif metric == 'prc':\n",
    "            plt.ylabel('Average precision')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.legend(loc='upper right')\n",
    "        plt.savefig('%s%s_%s_vs_%s' % (outdir, metric, pos_str, neg_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot one vs. rest results\n",
    "plot_ovr(actual_Y, Y_predicted_lst, mdlname_lst, GEN_LABELS, GEN_LAB_STR, outdir, 'auc')\n",
    "plot_ovr(actual_Y, Y_predicted_lst, mdlname_lst, GEN_LABELS, GEN_LAB_STR, outdir, 'prc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot one vs. one results\n",
    "plot_ovo(actual_Y, Y_predicted_lst, mdlname_lst, FINE_LABELS, FINE_LAB_STR, outdir, 'auc')\n",
    "plot_ovo(actual_Y, Y_predicted_lst, mdlname_lst, FINE_LABELS, FINE_LAB_STR, outdir, 'prc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
